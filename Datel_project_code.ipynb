{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Log in to your Hugging Face account to download models\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Q4xDSK94mFs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required Python libraries\n",
        "!pip install -q -U transformers accelerate peft trl bitsandbytes sentence-transformers faiss-cpu openpyxl streamlit"
      ],
      "metadata": {
        "id": "0GyqDb9_mSFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Datel_Project\""
      ],
      "metadata": {
        "id": "sCRlYOoONTR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Build assets"
      ],
      "metadata": {
        "id": "5IWhf9ilmHnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Datel_Project\"\n",
        "ASSETS_DIR = os.path.join(DRIVE_PROJECT_PATH, \"assets\")\n",
        "\n",
        "def create_enhanced_vector_database(main_df, history_df, model):\n",
        "    \"\"\"\n",
        "    Encodes the full conversation history of tickets into vectors\n",
        "    and saves them in a FAISS index for enhanced retrieval.\n",
        "    \"\"\"\n",
        "    logging.info(\"Creating ENHANCED vector database for solution retrieval...\")\n",
        "\n",
        "    # --- Step 1: Process and aggregate the history data ---\n",
        "    logging.info(\"Aggregating ticket conversation histories...\")\n",
        "    # Drop rows with no description\n",
        "    history_df = history_df.dropna(subset=['ACTIVITYDESC']).copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "\n",
        "    # This line fixes the TypeError by ensuring all items are strings before joining.\n",
        "    history_df['ACTIVITYDESC'] = history_df['ACTIVITYDESC'].astype(str)\n",
        "\n",
        "    history_df['CREATEDATE'] = pd.to_datetime(history_df['CREATEDATE'])\n",
        "    history_df = history_df.sort_values(by=['TICKETID', 'CREATEDATE'])\n",
        "\n",
        "    # Group by TICKETID and join all descriptions into a single text block\n",
        "    # This creates a full story for each ticket\n",
        "    aggregated_history = history_df.groupby('TICKETID')['ACTIVITYDESC'].apply(\n",
        "        lambda activities: \"\\n---\\n\".join(activities)\n",
        "    ).reset_index()\n",
        "    aggregated_history.rename(columns={'ACTIVITYDESC': 'FULL_HISTORY'}, inplace=True)\n",
        "\n",
        "    # --- Step 2: Merge aggregated history back into the main dataframe ---\n",
        "    logging.info(\"Merging full histories with main ticket data...\")\n",
        "    merged_df = pd.merge(main_df, aggregated_history, on='TICKETID', how='left')\n",
        "\n",
        "    # --- Step 3: Create a single \"knowledge\" column ---\n",
        "    # We prioritize the full history. If it doesn't exist, we fall back to the final SOLUTION.\n",
        "    merged_df['KNOWLEDGE_TEXT'] = merged_df['FULL_HISTORY'].fillna(merged_df['SOLUTION'])\n",
        "\n",
        "    # Filter out tickets that have no knowledge text at all\n",
        "    df_knowledge = merged_df.dropna(subset=['KNOWLEDGE_TEXT']).copy()\n",
        "    df_knowledge = df_knowledge[df_knowledge['KNOWLEDGE_TEXT'].str.strip() != '']\n",
        "\n",
        "    if df_knowledge.empty:\n",
        "        logging.warning(\"No knowledge text found to build the vector database. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # --- Step 4: Encode the new knowledge text and build the FAISS index ---\n",
        "    logging.info(f\"Encoding {len(df_knowledge)} enriched ticket knowledge documents...\")\n",
        "    knowledge_embeddings = model.encode(df_knowledge['KNOWLEDGE_TEXT'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "    index = faiss.IndexFlatL2(knowledge_embeddings.shape[1])\n",
        "    index.add(knowledge_embeddings.cpu().numpy())\n",
        "\n",
        "    # --- Step 5: Save the new assets ---\n",
        "    os.makedirs(ASSETS_DIR, exist_ok=True)\n",
        "    faiss.write_index(index, os.path.join(ASSETS_DIR, \"ticket_knowledge.index\"))\n",
        "    df_knowledge.to_pickle(os.path.join(ASSETS_DIR, \"ticket_knowledge_data.pkl\"))\n",
        "\n",
        "    logging.info(f\"✅ Enhanced vector database saved to '{ASSETS_DIR}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"--- Starting Enhanced Asset Building Process ---\")\n",
        "\n",
        "    # Define file paths\n",
        "    main_tickets_file = \"/content/drive/MyDrive/Datel_Project/Copy of Tickets_cleaned.xlsx\"\n",
        "    history_file = \"/content/drive/MyDrive/Datel_Project/Ticket history 3.xlsx\" # The new history file\n",
        "\n",
        "    try:\n",
        "        main_df = pd.read_excel(main_tickets_file)\n",
        "        history_df = pd.read_excel(history_file)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"FATAL: Could not find a data file. Error: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # Load the sentence transformer model for creating embeddings\n",
        "    retrieval_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Create the enhanced vector database using both dataframes\n",
        "    create_enhanced_vector_database(main_df, history_df, retrieval_model)\n",
        "\n",
        "    logging.info(\"--- Asset Building Complete ---\")\n",
        "    print(\"--- Asset Building Complete ---\")"
      ],
      "metadata": {
        "id": "A8LoFmhOI8i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Model Training\n",
        "##2.1 Mistral"
      ],
      "metadata": {
        "id": "S-g7X5kdEdrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Complete Training Script for the Mistral Model ---\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "import logging\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- Global Random Seed Settings ---\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"\n",
        "    Sets the random seeds for all relevant libraries to ensure reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # For multi-GPU setups.\n",
        "        # The following two lines are crucial for ensuring CUDA operations are deterministic.\n",
        "        # Note: This may slightly reduce computational speed.\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Call the function at the beginning of your script's main execution flow.\n",
        "set_seed(42)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Datel_Project\"\n",
        "MODELS_DIR = os.path.join(DRIVE_PROJECT_PATH, \"saved_models\")\n",
        "\n",
        "# --- DATA LOADING AND PREPARATION ---\n",
        "def load_and_prepare_data(filepath=\"/content/drive/MyDrive/Datel_Project/Copy of Tickets_cleaned.xlsx\"):\n",
        "    logging.info(f\"🔄 Loading and preparing data from '{filepath}'...\")\n",
        "    df = pd.read_excel(filepath)\n",
        "    df = df.drop(columns=['ACCOUNTID', 'ACCOUNT', 'TICKETID', 'ALTERNATEKEYSUFFIX'], errors='ignore')\n",
        "    df['SUBJECT'] = df['SUBJECT'].fillna('').astype(str)\n",
        "    df['PROBLEM'] = df['PROBLEM'].fillna('').astype(str)\n",
        "\n",
        "    # Use 'ISSUE' as a target column\n",
        "    for col in ['ISSUE', 'CATEGORY', 'URGENCYCODE']:\n",
        "        df[col] = df[col].fillna('Unknown').astype(str)\n",
        "\n",
        "    df['fullText'] = (df['SUBJECT'].str.strip() + \" | \" + df['PROBLEM'].str.strip()).str.strip()\n",
        "    df = df[df['fullText'].str.len() > 10]\n",
        "\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['URGENCYCODE'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['URGENCYCODE'])\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# --- Simplified Prompt Engineering ---\n",
        "def create_super_simple_prompt(row):\n",
        "    subject = str(row['SUBJECT']).strip()\n",
        "    problem = str(row['PROBLEM']).strip()\n",
        "\n",
        "    issue_value = str(row['ISSUE']).strip() if str(row['ISSUE']).strip() != 'nan' else 'Unknown'\n",
        "    category_value = str(row['CATEGORY']).strip()\n",
        "    urgency_value = str(row['URGENCYCODE']).strip()\n",
        "\n",
        "    return f\"\"\"Classify this ticket:\n",
        "\n",
        "Subject: {subject}\n",
        "Problem: {problem}\n",
        "\n",
        "Output this exact format:\n",
        "```json\n",
        "{{\n",
        "  \"ISSUE\": \"{issue_value}\",\n",
        "  \"CATEGORY\": \"{category_value}\",\n",
        "  \"URGENCYCODE\": \"{urgency_value}\"\n",
        "}}\n",
        "```\"\"\"\n",
        "\n",
        "# --- MODEL TRAINING ---\n",
        "def train_mistral_model(model_name, train_df, val_df=None):\n",
        "    logging.info(f\"🚀 Starting training for: {model_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_df_copy = train_df.copy()\n",
        "    train_df_copy['text'] = train_df_copy.apply(create_super_simple_prompt, axis=1)\n",
        "    train_dataset = Dataset.from_pandas(train_df_copy[['text']])\n",
        "\n",
        "    # Print some examples to see the format\n",
        "    print(\"=== Training Examples ===\")\n",
        "    for i in range(min(3, len(train_dataset))):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(train_dataset[i]['text'])\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, low_cpu_mem_usage=True)\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    output_dir = os.path.join(MODELS_DIR, f\"{model_name.split('/')[-1]}_optimized_v16\")\n",
        "    training_args = TrainingArguments(output_dir=output_dir, overwrite_output_dir=True, per_device_train_batch_size=1, gradient_accumulation_steps=16, gradient_checkpointing=True, learning_rate=5e-5, warmup_steps=100, weight_decay=0.01, num_train_epochs=3, save_strategy=\"epoch\", save_total_limit=1, logging_steps=50, report_to=\"none\", fp16=False, bf16=True, seed=42)\n",
        "\n",
        "    def formatting_func(example):\n",
        "        return example['text']\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        args=training_args,\n",
        "        processing_class=tokenizer,\n",
        "        peft_config=lora_config,\n",
        "        formatting_func=formatting_func,\n",
        "    )\n",
        "\n",
        "    train_result = trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Save training metrics\n",
        "    training_metrics = {\n",
        "        \"training_loss\": train_result.training_loss,\n",
        "        \"training_time_hours\": training_time/3600,\n",
        "        \"total_steps\": train_result.global_step,\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset_size\": len(train_dataset),\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_dir, \"training_metrics.json\"), 'w') as f:\n",
        "        json.dump(training_metrics, f, indent=2)\n",
        "\n",
        "    return trainer.model, tokenizer, output_dir\n",
        "\n",
        "# --- MODEL EVALUATION ---\n",
        "def evaluate_model_comprehensive(model, tokenizer, test_df, model_name, model_path):\n",
        "    logging.info(f\"🔄 Evaluating model: {model_name}\")\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    eval_df = test_df.sample(n=min(200, len(test_df)), random_state=42)\n",
        "\n",
        "    for _, row in eval_df.iterrows():\n",
        "        # This ensures a fair comparison with the model's text-based output.\n",
        "        true_labels.append({\n",
        "            \"ISSUE\": str(row['ISSUE']),\n",
        "            \"CATEGORY\": str(row['CATEGORY']),\n",
        "            \"URGENCYCODE\": str(row['URGENCYCODE'])\n",
        "        })\n",
        "\n",
        "        subject = str(row['SUBJECT']).strip()\n",
        "        problem = str(row['PROBLEM']).strip()\n",
        "\n",
        "        prompt = f\"\"\"Classify this ticket:\n",
        "\n",
        "Subject: {subject}\n",
        "Problem: {problem}\n",
        "\n",
        "Output this exact format:\n",
        "```json\n",
        "\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "\n",
        "        # This context manager handles the dtype conversion automatically.\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        try:\n",
        "            json_str = prediction_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "            pred_labels = json.loads(json_str)\n",
        "            predictions.append({\n",
        "                \"ISSUE\": str(pred_labels.get(\"ISSUE\", \"Unknown\")),\n",
        "                \"CATEGORY\": str(pred_labels.get(\"CATEGORY\", \"Unknown\")),\n",
        "                \"URGENCYCODE\": str(pred_labels.get(\"URGENCYCODE\", \"Unknown\"))\n",
        "            })\n",
        "        except (IndexError, json.JSONDecodeError):\n",
        "            predictions.append({\"ISSUE\": \"Parse Error\", \"CATEGORY\": \"Parse Error\", \"URGENCYCODE\": \"Parse Error\"})\n",
        "\n",
        "    # Evaluation\n",
        "    fields = [\"ISSUE\", \"CATEGORY\", \"URGENCYCODE\"]\n",
        "    results = {}\n",
        "    print(f\"\\n📊 {model_name} Evaluate result\")\n",
        "\n",
        "    for field in fields:\n",
        "        y_true = [label[field] for label in true_labels]\n",
        "        y_pred = [pred[field] for pred in predictions]\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        results[field] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"f1_score\": f1,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall\n",
        "        }\n",
        "        print(f\"\\n{field}:\\n  Accuracy: {accuracy:.4f}\\n  F1: {f1:.4f}\\n  precision: {precision:.4f}\\n  recall: {recall:.4f}\")\n",
        "\n",
        "    # overall result\n",
        "    overall_accuracy = np.mean([results[field][\"accuracy\"] for field in fields])\n",
        "    overall_f1 = np.mean([results[field][\"f1_score\"] for field in fields])\n",
        "    overall_precision = np.mean([results[field][\"precision\"] for field in fields])\n",
        "    overall_recall = np.mean([results[field][\"recall\"] for field in fields])\n",
        "\n",
        "    results[\"overall\"] = {\n",
        "        \"accuracy\": overall_accuracy,\n",
        "        \"f1_score\": overall_f1,\n",
        "        \"precision\": overall_precision,\n",
        "        \"recall\": overall_recall\n",
        "    }\n",
        "\n",
        "    print(f\"\\nOverall evaluation:\\n  Accuracy: {overall_accuracy:.4f}\\n  F1: {overall_f1:.4f}\\n  precision: {overall_precision:.4f}\\n  recall: {overall_recall:.4f}\")\n",
        "\n",
        "    with open(os.path.join(model_path, \"evaluation_results.json\"), 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_data, val_data, test_data = load_and_prepare_data()\n",
        "    if train_data is not None:\n",
        "        # Specify the Mistral model to be trained\n",
        "        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "        try:\n",
        "            trained_model, trained_tokenizer, model_path = train_mistral_model(model_name, train_data, val_data)\n",
        "            evaluate_model_comprehensive(trained_model, trained_tokenizer, test_data, model_name, model_path)\n",
        "            del trained_model, trained_tokenizer\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Error processing {model_name}: {str(e)}\", exc_info=True)\n"
      ],
      "metadata": {
        "id": "DjwbVsK9H4ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Gemma"
      ],
      "metadata": {
        "id": "0m5v2xde-yaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "import logging\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# --- Global Random Seed Settings ---\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"\n",
        "    Sets the random seeds for all relevant libraries to ensure reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # For multi-GPU setups.\n",
        "        # The following two lines are crucial for ensuring CUDA operations are deterministic.\n",
        "        # Note: This may slightly reduce computational speed.\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Call the function at the beginning of your script's main execution flow.\n",
        "set_seed(42)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Datel_Project\"\n",
        "MODELS_DIR = os.path.join(DRIVE_PROJECT_PATH, \"saved_models\")\n",
        "\n",
        "# --- DATA LOADING AND PREPARATION ---\n",
        "def load_and_prepare_data(filepath=\"/content/drive/MyDrive/Datel_Project/Copy of Tickets_cleaned.xlsx\"):\n",
        "    logging.info(f\"🔄 Loading and preparing data from '{filepath}'...\")\n",
        "    df = pd.read_excel(filepath)\n",
        "    df = df.drop(columns=['ACCOUNTID', 'ACCOUNT', 'TICKETID', 'ALTERNATEKEYSUFFIX'], errors='ignore')\n",
        "    df['SUBJECT'] = df['SUBJECT'].fillna('').astype(str)\n",
        "    df['PROBLEM'] = df['PROBLEM'].fillna('').astype(str)\n",
        "\n",
        "    # --- deal with missing value ---\n",
        "    df['ISSUE'] = df['ISSUE'].fillna('Unknown').astype(str)\n",
        "    df['CATEGORY'] = df['CATEGORY'].fillna('Unknown').astype(str)\n",
        "    df['URGENCYCODE'] = df['URGENCYCODE'].fillna('3').astype(str)  # set it as medium level\n",
        "\n",
        "    df['fullText'] = (df['SUBJECT'].str.strip() + \" | \" + df['PROBLEM'].str.strip()).str.strip()\n",
        "    df = df[df['fullText'].str.len() > 10]\n",
        "\n",
        "    # Print data analysis to understand categories\n",
        "    print(\"=== DATA ANALYSIS ===\")\n",
        "    print(\"\\nCATEGORY distribution:\")\n",
        "    print(df['CATEGORY'].value_counts())\n",
        "    print(\"\\nURGENCYCODE distribution:\")\n",
        "    print(df['URGENCYCODE'].value_counts())\n",
        "    print(\"\\nISSUE distribution (Top 10):\")\n",
        "    print(df['ISSUE'].value_counts().head(10))\n",
        "\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['URGENCYCODE'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['URGENCYCODE'])\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# --- Simplified Prompt Engineering ---\n",
        "def create_super_simple_prompt(row):\n",
        "    subject = str(row['SUBJECT']).strip()\n",
        "    problem = str(row['PROBLEM']).strip()\n",
        "\n",
        "    issue_value = str(row['ISSUE']).strip() if str(row['ISSUE']).strip() != 'nan' else 'Unknown'\n",
        "    category_value = str(row['CATEGORY']).strip()\n",
        "    urgency_value = str(row['URGENCYCODE']).strip()\n",
        "\n",
        "    return f\"\"\"Classify this ticket:\n",
        "\n",
        "Subject: {subject}\n",
        "Problem: {problem}\n",
        "\n",
        "Output this exact format:\n",
        "```json\n",
        "{{\n",
        "  \"ISSUE\": \"{issue_value}\",\n",
        "  \"CATEGORY\": \"{category_value}\",\n",
        "  \"URGENCYCODE\": \"{urgency_value}\"\n",
        "}}\n",
        "```\"\"\"\n",
        "\n",
        "# --- MODEL TRAINING ---\n",
        "def train_optimized_model(model_name, train_df, val_df=None):\n",
        "    logging.info(f\"🚀 Starting training for: {model_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_df_copy = train_df.copy()\n",
        "    # Use the simplified prompt\n",
        "    train_df_copy['text'] = train_df_copy.apply(create_super_simple_prompt, axis=1)\n",
        "    train_dataset = Dataset.from_pandas(train_df_copy[['text']])\n",
        "\n",
        "    # Print some examples to see the format\n",
        "    print(\"=== Training Examples ===\")\n",
        "    for i in range(min(3, len(train_dataset))):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(train_dataset[i]['text'])\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, low_cpu_mem_usage=True)\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    output_dir = os.path.join(MODELS_DIR, f\"{model_name.split('/')[-1]}_optimized\")  # v15 version\n",
        "    training_args = TrainingArguments(output_dir=output_dir, overwrite_output_dir=True, per_device_train_batch_size=1, gradient_accumulation_steps=16, gradient_checkpointing=True, learning_rate=5e-5, warmup_steps=100, weight_decay=0.01, num_train_epochs=3, save_strategy=\"epoch\", save_total_limit=1, logging_steps=50, report_to=\"none\", fp16=False, bf16=True, seed=42)  # 增加到3個epoch\n",
        "\n",
        "    def formatting_func(example):\n",
        "        return example['text']\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        args=training_args,\n",
        "        processing_class=tokenizer,\n",
        "        peft_config=lora_config,\n",
        "        formatting_func=formatting_func,\n",
        "    )\n",
        "\n",
        "    train_result = trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Save training metrics\n",
        "    training_metrics = {\n",
        "        \"training_loss\": train_result.training_loss,\n",
        "        \"training_time_hours\": training_time/3600,\n",
        "        \"total_steps\": train_result.global_step,\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset_size\": len(train_dataset),\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_dir, \"training_metrics.json\"), 'w') as f:\n",
        "        json.dump(training_metrics, f, indent=2)\n",
        "\n",
        "    return trainer.model, tokenizer, output_dir\n",
        "\n",
        "# --- MODEL EVALUATION ---\n",
        "def evaluate_model_comprehensive(model, tokenizer, test_df, model_name, model_path):\n",
        "    logging.info(f\"🔄 Evaluating model: {model_name}\")\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    eval_df = test_df.sample(n=min(200, len(test_df)), random_state=42)  # reduce training samples\n",
        "\n",
        "    for _, row in eval_df.iterrows():\n",
        "        true_labels.append({\"ISSUE\": row['ISSUE'], \"CATEGORY\": row['CATEGORY'], \"URGENCYCODE\": row['URGENCYCODE']})\n",
        "\n",
        "        subject = str(row['SUBJECT']).strip()\n",
        "        problem = str(row['PROBLEM']).strip()\n",
        "\n",
        "        # Use the format same as training\n",
        "        prompt = f\"\"\"Classify this ticket:\n",
        "\n",
        "Subject: {subject}\n",
        "Problem: {problem}\n",
        "\n",
        "Output this exact format:\n",
        "```json\n",
        "\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "        prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        try:\n",
        "            json_str = prediction_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "            pred_labels = json.loads(json_str)\n",
        "            predictions.append({\n",
        "                \"ISSUE\": pred_labels.get(\"ISSUE\", \"Unknown\"),\n",
        "                \"CATEGORY\": pred_labels.get(\"CATEGORY\", \"Unknown\"),\n",
        "                \"URGENCYCODE\": str(pred_labels.get(\"URGENCYCODE\", \"Unknown\"))\n",
        "            })\n",
        "        except (IndexError, json.JSONDecodeError):\n",
        "            predictions.append({\"ISSUE\": \"Parse Error\", \"CATEGORY\": \"Parse Error\", \"URGENCYCODE\": \"Parse Error\"})\n",
        "\n",
        "    # evaluation\n",
        "    fields = [\"ISSUE\", \"CATEGORY\", \"URGENCYCODE\"]\n",
        "    results = {}\n",
        "    print(f\"\\n📊 {model_name} Evaluate result\")\n",
        "\n",
        "    for field in fields:\n",
        "        y_true = [label[field] for label in true_labels]\n",
        "        y_pred = [pred[field] for pred in predictions]\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        results[field] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"f1_score\": f1,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall\n",
        "        }\n",
        "        print(f\"\\n{field}:\\n  Accuracy: {accuracy:.4f}\\n  F1: {f1:.4f}\\n  precision: {precision:.4f}\\n  recall: {recall:.4f}\")\n",
        "\n",
        "    # overall result\n",
        "    overall_accuracy = np.mean([results[field][\"accuracy\"] for field in fields])\n",
        "    overall_f1 = np.mean([results[field][\"f1_score\"] for field in fields])\n",
        "    overall_precision = np.mean([results[field][\"precision\"] for field in fields])\n",
        "    overall_recall = np.mean([results[field][\"recall\"] for field in fields])\n",
        "\n",
        "    results[\"overall\"] = {\n",
        "        \"accuracy\": overall_accuracy,\n",
        "        \"f1_score\": overall_f1,\n",
        "        \"precision\": overall_precision,\n",
        "        \"recall\": overall_recall\n",
        "    }\n",
        "\n",
        "    print(f\"\\nOverall evaluation:\\n  Accuracy: {overall_accuracy:.4f}\\n  F1: {overall_f1:.4f}\\n  precision: {overall_precision:.4f}\\n  recall: {overall_recall:.4f}\")\n",
        "\n",
        "    with open(os.path.join(model_path, \"evaluation_results.json\"), 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_data, val_data, test_data = load_and_prepare_data()\n",
        "    if train_data is not None:\n",
        "        models_to_run = [\"google/gemma-2b-it\"]  # run gemma first\n",
        "        for model_name in models_to_run:\n",
        "            try:\n",
        "                trained_model, trained_tokenizer, model_path = train_optimized_model(model_name, train_data, val_data)\n",
        "                evaluate_model_comprehensive(trained_model, trained_tokenizer, test_data, model_name, model_path)\n",
        "                del trained_model, trained_tokenizer\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                logging.error(f\"❌ Error processing {model_name}: {str(e)}\", exc_info=True)"
      ],
      "metadata": {
        "id": "ybpTiJs2uJ6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3 BERT"
      ],
      "metadata": {
        "id": "y9ylGN_MFAiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EvalPrediction\n",
        ")\n",
        "from datasets import Dataset\n",
        "import logging\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Mount Google Drive & Define Paths ---\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Datel_Project\"\n",
        "MODELS_DIR = os.path.join(DRIVE_PROJECT_PATH, \"saved_models\") # New version folder\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# --- DATA LOADING AND PREPARATION ---\n",
        "def load_and_prepare_data(filepath=\"/content/drive/MyDrive/Datel_Project/Copy of Tickets_cleaned.xlsx\"):\n",
        "    \"\"\"Loads and prepares data, splitting it into train, validation, and test sets.\"\"\"\n",
        "    logging.info(f\"🔄 Loading and preparing data from '{filepath}'...\")\n",
        "    df = pd.read_excel(filepath)\n",
        "    df = df.drop(columns=['ACCOUNTID', 'ACCOUNT', 'TICKETID', 'ALTERNATEKEYSUFFIX'], errors='ignore')\n",
        "\n",
        "    # Clean and fill missing values\n",
        "    df['SUBJECT'] = df['SUBJECT'].fillna('').astype(str)\n",
        "    df['PROBLEM'] = df['PROBLEM'].fillna('').astype(str)\n",
        "    df['ISSUE'] = df['ISSUE'].fillna('Unknown').astype(str)\n",
        "    df['CATEGORY'] = df['CATEGORY'].fillna('Unknown').astype(str)\n",
        "    df['URGENCYCODE'] = df['URGENCYCODE'].fillna('3').astype(str)\n",
        "\n",
        "    df['fullText'] = (df['SUBJECT'].str.strip() + \" | \" + df['PROBLEM'].str.strip()).str.strip()\n",
        "    df = df[df['fullText'].str.len() > 10]\n",
        "\n",
        "    df['labels'] = df.apply(\n",
        "        lambda row: [\n",
        "            f\"ISSUE:{row['ISSUE']}\",\n",
        "            f\"CATEGORY:{row['CATEGORY']}\",\n",
        "            f\"URGENCYCODE:{row['URGENCYCODE']}\"\n",
        "        ],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Create a 3-way split (train, validation, test)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['URGENCYCODE'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['URGENCYCODE'])\n",
        "\n",
        "    logging.info(f\"✅ Data loaded. Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)} rows.\")\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def evaluate_bert_macro(trainer, test_dataset, mlb, output_dir):\n",
        "    \"\"\"\n",
        "    Performs a detailed evaluation by calculating metrics for each category separately.\n",
        "    \"\"\"\n",
        "    logging.info(\"📊 Performing detailed, per-category (macro) evaluation...\")\n",
        "\n",
        "    # 1. Get model predictions\n",
        "    predictions_output = trainer.predict(test_dataset)\n",
        "    logits = predictions_output.predictions\n",
        "    true_labels_binary = predictions_output.label_ids\n",
        "\n",
        "    # 2. Convert logits to binary predictions\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(logits))\n",
        "    predictions_binary = np.zeros(probs.shape)\n",
        "    predictions_binary[np.where(probs > 0.5)] = 1\n",
        "\n",
        "    # 3. Decode binary predictions back to text labels using the binarizer\n",
        "    predicted_labels_text = mlb.inverse_transform(predictions_binary)\n",
        "    true_labels_text = mlb.inverse_transform(true_labels_binary)\n",
        "\n",
        "    # 4. Parse the text labels into dictionaries\n",
        "    def parse_labels(label_tuples):\n",
        "        parsed = []\n",
        "        for label_tuple in label_tuples:\n",
        "            d = {\"ISSUE\": \"Unknown\", \"CATEGORY\": \"Unknown\", \"URGENCYCODE\": \"Unknown\"}\n",
        "            for label in label_tuple:\n",
        "                key, value = label.split(\":\", 1)\n",
        "                d[key] = value\n",
        "            parsed.append(d)\n",
        "        return parsed\n",
        "\n",
        "    y_pred_parsed = parse_labels(predicted_labels_text)\n",
        "    y_true_parsed = parse_labels(true_labels_text)\n",
        "\n",
        "    # 5. Calculate metrics for each field\n",
        "    fields = [\"ISSUE\", \"CATEGORY\", \"URGENCYCODE\"]\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 DETAILED BERT EVALUATION (MACRO)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for field in fields:\n",
        "        y_true = [d[field] for d in y_true_parsed]\n",
        "        y_pred = [d[field] for d in y_pred_parsed]\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        results[field] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"f1_score\": f1,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall\n",
        "        }\n",
        "        print(f\"\\n{field} Classification:\")\n",
        "        print(f\"  - Accuracy:  {accuracy:.4f}\")\n",
        "        print(f\"  - F1 Score:  {f1:.4f}\")\n",
        "        print(f\"  - Precision: {precision:.4f}\")\n",
        "        print(f\"  - Recall:    {recall:.4f}\")\n",
        "\n",
        "    # 6. Calculate and print overall macro-average results\n",
        "    overall_accuracy = np.mean([results[field][\"accuracy\"] for field in fields])\n",
        "    overall_f1 = np.mean([results[field][\"f1_score\"] for field in fields])\n",
        "    overall_precision = np.mean([results[field][\"precision\"] for field in fields])\n",
        "    overall_recall = np.mean([results[field][\"recall\"] for field in fields])\n",
        "\n",
        "    results[\"overall_macro_average\"] = {\n",
        "        \"accuracy\": overall_accuracy,\n",
        "        \"f1_score\": overall_f1,\n",
        "        \"precision\": overall_precision,\n",
        "        \"recall\": overall_recall\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"OVERALL MACRO-AVERAGE:\")\n",
        "    print(f\"  - Avg Accuracy: {overall_accuracy:.4f}\")\n",
        "    print(f\"  - Avg F1 Score: {overall_f1:.4f}\")\n",
        "    print(f\"  - Avg Precision: {overall_precision:.4f}\")\n",
        "    print(f\"  - Avg Recall: {overall_recall:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 7. Save the detailed results to a new file\n",
        "    results_path = os.path.join(output_dir, \"evaluation_results_detailed.json\")\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    logging.info(f\"💾 Detailed evaluation results saved to {results_path}\")\n",
        "\n",
        "\n",
        "# --- BERT MODEL TRAINING & EVALUATION ---\n",
        "def train_bert_classifier(train_df, val_df, test_df, model_name=\"bert-base-uncased\"):\n",
        "    logging.info(f\"🚀 Starting BERT training for: {model_name}\")\n",
        "\n",
        "    # Encode Labels\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    train_labels = mlb.fit_transform(train_df['labels'])\n",
        "    val_labels = mlb.transform(val_df['labels'])\n",
        "    test_labels = mlb.transform(test_df['labels'])\n",
        "\n",
        "    # Save the Binarizer\n",
        "    output_dir = os.path.join(MODELS_DIR, f\"{model_name.split('/')[-1]}_triage\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    binarizer_path = os.path.join(output_dir, \"mlb_binarizer.joblib\")\n",
        "    joblib.dump(mlb, binarizer_path)\n",
        "    logging.info(f\"💾 Label binarizer saved to {binarizer_path}\")\n",
        "\n",
        "    # Tokenize Text\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    train_encodings = tokenizer(train_df['fullText'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "    val_encodings = tokenizer(val_df['fullText'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "    test_encodings = tokenizer(test_df['fullText'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Create PyTorch Datasets\n",
        "    class TicketDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "            return item\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = TicketDataset(train_encodings, train_labels)\n",
        "    val_dataset = TicketDataset(val_encodings, val_labels)\n",
        "    test_dataset = TicketDataset(test_encodings, test_labels)\n",
        "\n",
        "    # Define Micro-Average Metrics for Trainer\n",
        "    def compute_metrics_micro(p: EvalPrediction):\n",
        "        logits = p.predictions\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.Tensor(logits))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs > 0.5)] = 1\n",
        "        f1_micro = f1_score(y_true=p.label_ids, y_pred=predictions, average=\"micro\")\n",
        "        return {\"f1_micro\": f1_micro}\n",
        "\n",
        "    # Configure and Train the Model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(mlb.classes_), problem_type=\"multi_label_classification\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics_micro,\n",
        "    )\n",
        "\n",
        "    logging.info(\"🏋️‍♀️ Fine-tuning BERT model...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # --- Final Evaluation and Saving ---\n",
        "    logging.info(\"📊 Performing final micro-average evaluation...\")\n",
        "    eval_results_micro = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 FINAL BERT EVALUATION (MICRO-AVERAGE)\")\n",
        "    print(\"=\"*50)\n",
        "    for key, value in eval_results_micro.items():\n",
        "        print(f\"  - {key}: {value:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    results_path_micro = os.path.join(output_dir, \"evaluation_results_micro.json\")\n",
        "    with open(results_path_micro, 'w') as f:\n",
        "        json.dump(eval_results_micro, f, indent=2)\n",
        "    logging.info(f\"💾 Micro-average results saved to {results_path_micro}\")\n",
        "\n",
        "\n",
        "    evaluate_bert_macro(trainer, test_dataset, mlb, output_dir)\n",
        "\n",
        "    # Save the final model and tokenizer\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    logging.info(f\"✅ Model, tokenizer, and binarizer saved to {output_dir}\")\n",
        "\n",
        "# --- MAIN EXECUTION SCRIPT ---\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"🚀 Starting BERT Training and Evaluation Pipeline\")\n",
        "    if not torch.cuda.is_available():\n",
        "        logging.error(\"❌ GPU not available. This script requires a GPU.\")\n",
        "    else:\n",
        "        logging.info(f\"✅ GPU detected: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "    train_data, val_data, test_data = load_and_prepare_data()\n",
        "    if train_data is not None:\n",
        "        train_bert_classifier(train_data, val_data, test_data)\n",
        "        logging.info(\"🎉 BERT pipeline completed successfully.\")\n"
      ],
      "metadata": {
        "id": "8m6LWhiXEGjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Prototype\n",
        "Write 3_prototype_app_improved.py and then run it in your ngork"
      ],
      "metadata": {
        "id": "woFGfGt8YWws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 3_prototype_app_improved.py\n",
        "# --- Complete Prototype Script with Enhanced RAG and Stable JSON Output ---\n",
        "import streamlit as st\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- CONFIGURATION & SETUP ---\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Datel_Project\"\n",
        "ASSETS_DIR = os.path.join(DRIVE_PROJECT_PATH, \"assets\")\n",
        "MISTRAL_MODEL_PATH = os.path.join(DRIVE_PROJECT_PATH, \"saved_models\", \"Mistral-7B-Instruct-v0.2_optimized_v16\")\n",
        "\n",
        "# --- RESOURCE LOADING (CACHED FOR PERFORMANCE) ---\n",
        "@st.cache_resource\n",
        "def load_all_models_and_assets():\n",
        "    \"\"\"\n",
        "    Loads all necessary models and assets for the ENHANCED RAG pipeline.\n",
        "    ALSO dynamically builds the validation categories from the data.\n",
        "    \"\"\"\n",
        "    assets = {}\n",
        "    st.info(\"🔄 Loading all required AI models and assets...\")\n",
        "\n",
        "    # 1. Load ENHANCED RAG Retriever Assets\n",
        "    try:\n",
        "        assets['faiss_index'] = faiss.read_index(os.path.join(ASSETS_DIR, \"ticket_knowledge.index\"))\n",
        "        ticket_df = pd.read_pickle(os.path.join(ASSETS_DIR, \"ticket_knowledge_data.pkl\"))\n",
        "\n",
        "        # Handle NaN values in the 'ISSUE' column by replacing them with 'Unknown'\n",
        "        ticket_df['ISSUE'] = ticket_df['ISSUE'].fillna('Unknown')\n",
        "\n",
        "        assets['ticket_data'] = ticket_df\n",
        "        assets['retrieval_model'] = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        st.write(\"✅ Enhanced Retriever assets loaded.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load retriever assets from {ASSETS_DIR}. Please run the '1_build_assets.py' script first. Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # 2. Dynamically create the valid categories from the loaded data\n",
        "    try:\n",
        "        valid_categories = {\n",
        "            \"ISSUE\": sorted(ticket_df['ISSUE'].unique().tolist()), # Sorted for consistency\n",
        "            \"CATEGORY\": [\"S1000v4\", \"S1000v3\", \"S1000\", \"S1000v4/CRM\", \"S1000/CRM\"],\n",
        "            \"URGENCYCODE\": [\"1\", \"3\", \"4\", \"5\"]\n",
        "        }\n",
        "        st.write(f\"✅ Dynamically loaded {len(valid_categories['ISSUE'])} unique ISSUE types.\")\n",
        "    except KeyError as e:\n",
        "        st.error(f\"Error: Column '{e}' not found in 'ticket_knowledge_data.pkl'. Cannot create dynamic categories.\")\n",
        "        return None, None\n",
        "\n",
        "    # 3. Load Mistral (for both Classification and Generation)\n",
        "    try:\n",
        "        mistral_base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "        mistral_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            mistral_base_model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        assets['mistral_tokenizer'] = AutoTokenizer.from_pretrained(mistral_base_model_name)\n",
        "        if assets['mistral_tokenizer'].pad_token is None:\n",
        "            assets['mistral_tokenizer'].pad_token = assets['mistral_tokenizer'].eos_token\n",
        "        assets['mistral_model'] = PeftModel.from_pretrained(mistral_base_model, MISTRAL_MODEL_PATH)\n",
        "        assets['mistral_model'].eval()\n",
        "        st.write(\"✅ Mistral model loaded.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to load Mistral model from {MISTRAL_MODEL_PATH}. Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    st.success(\"🎉 All AI components are loaded and ready!\")\n",
        "    return assets, valid_categories\n",
        "\n",
        "# --- IMPROVED JSON PARSING FUNCTIONS ---\n",
        "def extract_json_from_text(text):\n",
        "    \"\"\"\n",
        "    Robustly extracts a JSON object from a string, trying various patterns.\n",
        "    \"\"\"\n",
        "    # Pattern 1: Code block with ```json\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    # Pattern 2: Any code block ```\n",
        "    match = re.search(r'```\\s*(\\{.*?\\})\\s*```', text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    # Pattern 3: A standalone JSON object\n",
        "    match = re.search(r'(\\{.*?\\})', text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "def validate_and_fix_classification(classification_dict, valid_categories):\n",
        "    \"\"\"\n",
        "    Validates classification results against the allowed values and fixes them.\n",
        "    \"\"\"\n",
        "    if not isinstance(classification_dict, dict):\n",
        "        return {\"ISSUE\": \"Unknown\", \"CATEGORY\": \"S1000v4\", \"URGENCYCODE\": \"3\"}\n",
        "\n",
        "    result = {}\n",
        "    for field, valid_values in valid_categories.items():\n",
        "        predicted_value = str(classification_dict.get(field, \"\")).strip()\n",
        "\n",
        "        if predicted_value in valid_values:\n",
        "            result[field] = predicted_value\n",
        "        else:\n",
        "            # Fallback to the most common value if validation fails\n",
        "            if field == \"ISSUE\":\n",
        "                result[field] = \"Unknown\"\n",
        "            elif field == \"CATEGORY\":\n",
        "                result[field] = \"S1000v4\"\n",
        "            else: # URGENCYCODE\n",
        "                result[field] = \"3\"\n",
        "    return result\n",
        "\n",
        "# --- BACKEND LOGIC ---\n",
        "@torch.no_grad()\n",
        "def run_mistral_classification(subject, problem, assets, valid_categories, max_retries=3):\n",
        "    \"\"\"\n",
        "    Uses the fine-tuned Mistral model to classify the ticket with improved stability.\n",
        "    \"\"\"\n",
        "    model = assets['mistral_model']\n",
        "    tokenizer = assets['mistral_tokenizer']\n",
        "\n",
        "    # Dynamically create the valid values list for the prompt\n",
        "    issue_list_str = \", \".join(valid_categories['ISSUE'])\n",
        "    category_list_str = \", \".join(valid_categories['CATEGORY'])\n",
        "    urgency_list_str = \", \".join(valid_categories['URGENCYCODE'])\n",
        "\n",
        "    prompt = f\"\"\"[INST] You are a technical support ticket classifier. Your task is to analyze the ticket and respond ONLY with a JSON object in the specified format.\n",
        "\n",
        "Valid values based on historical data:\n",
        "- ISSUE: {issue_list_str}\n",
        "- CATEGORY: {category_list_str}\n",
        "- URGENCYCODE: {urgency_list_str} (1=High, 3=Normal, 5=Low)\n",
        "\n",
        "Ticket Details:\n",
        "Subject: {subject}\n",
        "Problem: {problem}\n",
        "\n",
        "Respond with ONLY the JSON object in the following format:\n",
        "```json\n",
        "{{\n",
        "  \"ISSUE\": \"value\",\n",
        "  \"CATEGORY\": \"value\",\n",
        "  \"URGENCYCODE\": \"value\"\n",
        "}}\n",
        "```\n",
        "[/INST]\n",
        "\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=150,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "            prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            # Isolate the response part of the text\n",
        "            response_text = prediction_text.split(\"[/INST]\")[-1]\n",
        "\n",
        "            extracted_json = extract_json_from_text(response_text)\n",
        "            if extracted_json:\n",
        "                validated_result = validate_and_fix_classification(extracted_json, valid_categories)\n",
        "                validated_result[\"raw_output\"] = response_text\n",
        "                validated_result[\"attempt\"] = attempt + 1\n",
        "                return validated_result\n",
        "\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Classification attempt {attempt + 1} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Final fallback if all retries fail\n",
        "    return {\n",
        "        \"ISSUE\": \"Unknown\",\n",
        "        \"CATEGORY\": \"S1000v4\",\n",
        "        \"URGENCYCODE\": \"3\",\n",
        "        \"error\": \"Failed to generate valid classification after all attempts.\",\n",
        "        \"raw_output\": \"No valid output generated.\"\n",
        "    }\n",
        "\n",
        "def find_similar_tickets(text, assets, k=5):\n",
        "    \"\"\"Uses the vector database to find the top k similar historical tickets.\"\"\"\n",
        "    retrieval_model = assets['retrieval_model']\n",
        "    index = assets['faiss_index']\n",
        "    ticket_data = assets['ticket_data']\n",
        "    query_embedding = retrieval_model.encode([text])\n",
        "    _, indices = index.search(query_embedding, k)\n",
        "    return ticket_data.iloc[indices[0]]\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_mistral_solution_generation(subject, problem, similar_tickets_df, assets):\n",
        "    \"\"\"\n",
        "    Uses Mistral to generate a solution based on multiple historical contexts.\n",
        "    \"\"\"\n",
        "    model = assets['mistral_model']\n",
        "    tokenizer = assets['mistral_tokenizer']\n",
        "    combined_context = \"\"\n",
        "    num_cases_to_use = min(3, len(similar_tickets_df))\n",
        "\n",
        "    for i in range(num_cases_to_use):\n",
        "        ticket = similar_tickets_df.iloc[i]\n",
        "        case_context = ticket['KNOWLEDGE_TEXT']\n",
        "        if len(case_context) > 800:\n",
        "            case_context = case_context[:800] + \"...\"\n",
        "        combined_context += f\"\\n--- HISTORICAL CASE {i+1}: {ticket['SUBJECT']} ---\\n{case_context}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"[INST] You are an expert IT support assistant. Analyze the new ticket below and provide a step-by-step solution by synthesizing insights from the provided historical cases.\n",
        "\n",
        "MULTIPLE SIMILAR RESOLVED HISTORICAL CASES:\n",
        "{combined_context}\n",
        "NEW TICKET TO RESOLVE:\n",
        "Subject: {subject}\n",
        "Problem: {problem}\n",
        "\n",
        "Based on the patterns and solutions from the historical cases, provide a step-by-step solution that combines the best practices from these resolved tickets. Focus on actionable steps. [/INST]\n",
        "Based on the historical cases, here is a recommended solution:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3072).to(model.device)\n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    prediction_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    solution = prediction_text.split(\"[/INST]\")[-1].strip()\n",
        "    return solution\n",
        "\n",
        "# --- MAIN TRIAGE PIPELINE ---\n",
        "def run_full_triage_pipeline(subject, problem, assets, valid_categories):\n",
        "    \"\"\"Orchestrates the entire enhanced hybrid RAG process.\"\"\"\n",
        "    full_text = f\"{subject} | {problem}\"\n",
        "    classification_results = run_mistral_classification(subject, problem, assets, valid_categories)\n",
        "    similar_tickets_df = find_similar_tickets(full_text, assets, k=5)\n",
        "    solution_suggestion = run_mistral_solution_generation(subject, problem, similar_tickets_df, assets)\n",
        "    return {\n",
        "        \"classification\": classification_results,\n",
        "        \"suggested_solution\": solution_suggestion,\n",
        "        \"relevant_tickets\": similar_tickets_df\n",
        "    }\n",
        "\n",
        "# --- STREAMLIT UI ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Datel AI Triage System\")\n",
        "st.title(\"🤖 Datel AI Triage System (Enhanced RAG Prototype)\")\n",
        "\n",
        "assets, valid_categories = load_all_models_and_assets()\n",
        "\n",
        "if assets and valid_categories:\n",
        "    col1, col2 = st.columns([1, 1.5])\n",
        "    with col1:\n",
        "        st.header(\"New Ticket Entry\")\n",
        "        subject = st.text_input(\"Subject\", \"Example: Batch posting failed\")\n",
        "        problem = st.text_area(\"Problem Description\", \"Example: The PI batch 7268 failed to post.\", height=200)\n",
        "        if st.button(\"Triage Ticket\", type=\"primary\", use_container_width=True):\n",
        "            if subject and problem:\n",
        "                with st.spinner(\"🧠 AI is analyzing the ticket...\"):\n",
        "                    st.session_state.triage_result = run_full_triage_pipeline(subject, problem, assets, valid_categories)\n",
        "            else:\n",
        "                st.warning(\"Please enter both a subject and a problem description.\")\n",
        "    with col2:\n",
        "        st.header(\"AI Triage Recommendation\")\n",
        "        if 'triage_result' in st.session_state:\n",
        "            result = st.session_state.triage_result\n",
        "            st.subheader(\"1. AI Suggested Classification\")\n",
        "            classification = result['classification']\n",
        "            col2a, col2b, col2c = st.columns(3)\n",
        "            with col2a:\n",
        "                st.metric(\"Issue Type\", classification.get('ISSUE', 'N/A'))\n",
        "            with col2b:\n",
        "                st.metric(\"Category\", classification.get('CATEGORY', 'N/A'))\n",
        "            with col2c:\n",
        "                st.metric(\"Urgency\", classification.get('URGENCYCODE', 'N/A'))\n",
        "\n",
        "            if 'error' in classification:\n",
        "                with st.expander(\"🔍 Debug Information\"):\n",
        "                    st.warning(f\"Classification Issues: {classification['error']}\")\n",
        "                    st.text(f\"Raw Output: {classification.get('raw_output', 'N/A')}\")\n",
        "                    st.text(f\"Attempts Made: {classification.get('attempt', 'N/A')}\")\n",
        "\n",
        "            st.subheader(\"2. AI Suggested Solution (Multi-Case Analysis)\")\n",
        "            st.info(\"💡 This solution combines insights from the top 3 most similar historical cases.\")\n",
        "            st.markdown(result['suggested_solution'])\n",
        "\n",
        "            st.subheader(\"3. Top 5 Relevant Historical Tickets\")\n",
        "            st.caption(\"📊 Cases 1-3 were used for solution generation.\")\n",
        "            display_df = result['relevant_tickets'][['TICKETID', 'SUBJECT', 'KNOWLEDGE_TEXT']].copy()\n",
        "            display_df['KNOWLEDGE_TEXT'] = display_df['KNOWLEDGE_TEXT'].str[:200] + '...'\n",
        "            display_df['USED_FOR_SOLUTION'] = ['✓ Used', '✓ Used', '✓ Used', 'Reference', 'Reference']\n",
        "            st.dataframe(display_df[['TICKETID', 'SUBJECT', 'USED_FOR_SOLUTION', 'KNOWLEDGE_TEXT']], use_container_width=True)\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Validation & Feedback\")\n",
        "            feedback_cols = st.columns(2)\n",
        "            with feedback_cols[0]:\n",
        "                if st.button(\"👍 Recommendation was helpful\", use_container_width=True):\n",
        "                    st.success(\"Thank you for your feedback!\")\n",
        "            with feedback_cols[1]:\n",
        "                if st.button(\"👎 Recommendation was not helpful\", use_container_width=True):\n",
        "                    st.warning(\"Thank you for your feedback! We'll use this to improve our system.\")\n",
        "else:\n",
        "    st.error(\"Application failed to load. Please check the logs for errors related to model or asset loading.\")"
      ],
      "metadata": {
        "id": "G7MzZn48YZQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- use ngrok to run App ---\n",
        "\n",
        "# 1. install ngrok\n",
        "!pip install -q pyngrok\n",
        "\n",
        "# 2. write App file (if haven't done)\n",
        "#%%writefile 3_prototype_app.py\n",
        "# paste prototype_app code\n",
        "\n",
        "# 3. run ngrok\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "\n",
        "# copy the Authtoken from your ngork website\n",
        "AUTHTOKEN = \"\"\n",
        "\n",
        "os.system(f\"ngrok config add-authtoken {AUTHTOKEN}\")\n",
        "\n",
        "# sun Streamlit App in the background\n",
        "!streamlit run 3_prototype_app_improved.py &>/dev/null&\n",
        "\n",
        "# build tunnels and get the public link\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"✅ Your Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "id": "TXfqRhEpYpTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figures"
      ],
      "metadata": {
        "id": "yead7VXbmH-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Data provided by the user\n",
        "data = {\n",
        "    'Mistral': {\n",
        "        \"ISSUE\": {\n",
        "            \"accuracy\": 0.825,\n",
        "            \"f1_score\": 0.821400200722,\n",
        "            \"precision\": 0.8275517241379311,\n",
        "            \"recall\": 0.825\n",
        "        },\n",
        "        \"CATEGORY\": {\n",
        "            \"accuracy\": 0.78,\n",
        "            \"f1_score\": 0.6999106082827012,\n",
        "            \"precision\": 0.8248969072164948,\n",
        "            \"recall\": 0.78\n",
        "        },\n",
        "        \"URGENCYCODE\": {\n",
        "            \"accuracy\": 0.895,\n",
        "            \"f1_score\": 0.8658901098901097,\n",
        "            \"precision\": 0.8879166666666667,\n",
        "            \"recall\": 0.895\n",
        "        },\n",
        "        \"overall\": {\n",
        "            \"accuracy\": 0.8333333333333334,\n",
        "            \"f1_score\": 0.7957336396316036,\n",
        "            \"precision\": 0.8467884326736975,\n",
        "            \"recall\": 0.8333333333333334\n",
        "        }\n",
        "    },\n",
        "    'Bert': {\n",
        "        \"ISSUE\": {\n",
        "            \"accuracy\": 0.7282051282051282,\n",
        "            \"f1_score\": 0.6249495058278494,\n",
        "            \"precision\": 0.6057959401709402,\n",
        "            \"recall\": 0.7282051282051282\n",
        "        },\n",
        "        \"CATEGORY\": {\n",
        "            \"accuracy\": 0.8012820512820513,\n",
        "            \"f1_score\": 0.7233631687664744,\n",
        "            \"precision\": 0.6901547822017183,\n",
        "            \"recall\": 0.8012820512820513\n",
        "        },\n",
        "        \"URGENCYCODE\": {\n",
        "            \"accuracy\": 0.8910256410256411,\n",
        "            \"f1_score\": 0.851186964762342,\n",
        "            \"precision\": 0.8838558360042734,\n",
        "            \"recall\": 0.8910256410256411\n",
        "        },\n",
        "        \"overall_macro_average\": {\n",
        "            \"accuracy\": 0.8068376068376067,\n",
        "            \"f1_score\": 0.7331665464522219,\n",
        "            \"precision\": 0.726602186125644,\n",
        "            \"recall\": 0.8068376068376067\n",
        "        }\n",
        "    },\n",
        "    'Gemma': {\n",
        "        \"ISSUE\": {\n",
        "            \"accuracy\": 0.825,\n",
        "            \"f1_score\": 0.8184075448361163,\n",
        "            \"precision\": 0.8214041666666665,\n",
        "            \"recall\": 0.825\n",
        "        },\n",
        "        \"CATEGORY\": {\n",
        "            \"accuracy\": 0.77,\n",
        "            \"f1_score\": 0.6839198286413708,\n",
        "            \"precision\": 0.6301975945017183,\n",
        "            \"recall\": 0.77\n",
        "        },\n",
        "        \"URGENCYCODE\": {\n",
        "            \"accuracy\": 0.89,\n",
        "            \"f1_score\": 0.8572612886859461,\n",
        "            \"precision\": 0.8839248704663213,\n",
        "            \"recall\": 0.89\n",
        "        },\n",
        "        \"overall\": {\n",
        "            \"accuracy\": 0.8283333333333333,\n",
        "            \"f1_score\": 0.7865295540544777,\n",
        "            \"precision\": 0.7785088772115687,\n",
        "            \"recall\": 0.8283333333333333\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Standardize Bert's overall key\n",
        "if 'overall_macro_average' in data['Bert']:\n",
        "    data['Bert']['overall'] = data['Bert'].pop('overall_macro_average')\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "records = []\n",
        "for model_name, model_data in data.items():\n",
        "    for category, metrics in model_data.items():\n",
        "        for metric_name, value in metrics.items():\n",
        "            records.append({\n",
        "                'Model': model_name,\n",
        "                'Category': category,\n",
        "                'Metric': metric_name,\n",
        "                'Value': value\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Plotting in a single figure with subplots\n",
        "metrics_to_plot = ['accuracy', 'f1_score', 'precision', 'recall']\n",
        "models = df['Model'].unique()\n",
        "categories = df['Category'].unique()\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14)) # 2x2 subplots\n",
        "axes = axes.flatten() # Flatten the axes array for easy iteration\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Create the grouped bar chart on the current subplot\n",
        "    subset_df = df[df['Metric'] == metric]\n",
        "    pivot_df = subset_df.pivot(index='Category', columns='Model', values='Value')\n",
        "\n",
        "    # Ensure consistent order of categories\n",
        "    pivot_df = pivot_df.reindex(categories)\n",
        "\n",
        "    pivot_df.plot(kind='bar', ax=ax, width=0.8)\n",
        "\n",
        "    # Customize subplot\n",
        "    ax.set_title(f'Model Performance: {metric.replace(\"_\", \" \").title()}', fontsize=14, pad=10)\n",
        "    ax.set_ylabel('Score', fontsize=11)\n",
        "    ax.set_xlabel('Category', fontsize=11)\n",
        "    ax.tick_params(axis='x', rotation=0, labelsize=9)\n",
        "    ax.tick_params(axis='y', labelsize=9)\n",
        "    ax.set_ylim(0, 1.0)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', fontsize=7, padding=3)\n",
        "\n",
        "# Add a single legend for the entire figure\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "# Place the legend in the upper middle\n",
        "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=len(models), title='Model', fontsize=10, title_fontsize=11)\n",
        "\n",
        "# Remove individual subplot legends\n",
        "for ax in axes:\n",
        "    ax.get_legend().remove()\n",
        "\n",
        "# Adjusted tight_layout to make more space for the legend\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95]) # Increased space at the top\n",
        "\n",
        "\n",
        "# Save the figure\n",
        "file_name = 'overall_model_comparison.png'\n",
        "plt.savefig(file_name, bbox_inches='tight') # Use bbox_inches='tight' to include legend\n",
        "\n",
        "\n",
        "print(f\"Chart saved as {file_name}\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JHJDHjtDmHiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Data provided by the user\n",
        "data = {\n",
        "    'Mistral': {\n",
        "        \"ISSUE\": {\n",
        "            \"accuracy\": 0.825,\n",
        "            \"f1_score\": 0.821400200722,\n",
        "            \"precision\": 0.8275517241379311,\n",
        "            \"recall\": 0.825\n",
        "        },\n",
        "        \"CATEGORY\": {\n",
        "            \"accuracy\": 0.78,\n",
        "            \"f1_score\": 0.6999106082827012,\n",
        "            \"precision\": 0.8248969072164948,\n",
        "            \"recall\": 0.78\n",
        "        },\n",
        "        \"URGENCYCODE\": {\n",
        "            \"accuracy\": 0.895,\n",
        "            \"f1_score\": 0.8658901098901097,\n",
        "            \"precision\": 0.8879166666666667,\n",
        "            \"recall\": 0.895\n",
        "        },\n",
        "        \"overall\": {\n",
        "            \"accuracy\": 0.8333333333333334,\n",
        "            \"f1_score\": 0.7957336396316036,\n",
        "            \"precision\": 0.8467884326736975,\n",
        "            \"recall\": 0.8333333333333334\n",
        "        }\n",
        "    },\n",
        "    'Bert': {\n",
        "        \"ISSUE\": {\n",
        "            \"accuracy\": 0.7282051282051282,\n",
        "            \"f1_score\": 0.6249495058278494,\n",
        "            \"precision\": 0.6057959401709402,\n",
        "            \"recall\": 0.7282051282051282\n",
        "        },\n",
        "        \"CATEGORY\": {\n",
        "            \"accuracy\": 0.8012820512820513,\n",
        "            \"f1_score\": 0.7233631687664744,\n",
        "            \"precision\": 0.6901547822017183,\n",
        "            \"recall\": 0.8012820512820513\n",
        "        },\n",
        "        \"URGENCYCODE\": {\n",
        "            \"accuracy\": 0.8910256410256411,\n",
        "            \"f1_score\": 0.851186964762342,\n",
        "            \"precision\": 0.8838558360042734,\n",
        "            \"recall\": 0.8910256410256411\n",
        "        },\n",
        "        \"overall_macro_average\": {\n",
        "            \"accuracy\": 0.8068376068376067,\n",
        "            \"f1_score\": 0.7331665464522219,\n",
        "            \"precision\": 0.726602186125644,\n",
        "            \"recall\": 0.8068376068376067\n",
        "        }\n",
        "    },\n",
        "    'Gemma': {\n",
        "        \"ISSUE\": {\n",
        "            \"accuracy\": 0.825,\n",
        "            \"f1_score\": 0.8184075448361163,\n",
        "            \"precision\": 0.8214041666666665,\n",
        "            \"recall\": 0.825\n",
        "        },\n",
        "        \"CATEGORY\": {\n",
        "            \"accuracy\": 0.77,\n",
        "            \"f1_score\": 0.6839198286413708,\n",
        "            \"precision\": 0.6301975945017183,\n",
        "            \"recall\": 0.77\n",
        "        },\n",
        "        \"URGENCYCODE\": {\n",
        "            \"accuracy\": 0.89,\n",
        "            \"f1_score\": 0.8572612886859461,\n",
        "            \"precision\": 0.8839248704663213,\n",
        "            \"recall\": 0.89\n",
        "        },\n",
        "        \"overall\": {\n",
        "            \"accuracy\": 0.8283333333333333,\n",
        "            \"f1_score\": 0.7865295540544777,\n",
        "            \"precision\": 0.7785088772115687,\n",
        "            \"recall\": 0.8283333333333333\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Standardize Bert's overall key\n",
        "if 'overall_macro_average' in data['Bert']:\n",
        "    data['Bert']['overall'] = data['Bert'].pop('overall_macro_average')\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "records = []\n",
        "for model_name, model_data in data.items():\n",
        "    for category, metrics in model_data.items():\n",
        "        for metric_name, value in metrics.items():\n",
        "            records.append({\n",
        "                'Model': model_name,\n",
        "                'Category': category,\n",
        "                'Metric': metric_name,\n",
        "                'Value': value\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Plotting\n",
        "metrics_to_plot = df['Metric'].unique()\n",
        "models = df['Model'].unique()\n",
        "categories = df['Category'].unique()\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Create the grouped bar chart\n",
        "    subset_df = df[df['Metric'] == metric]\n",
        "    pivot_df = subset_df.pivot(index='Category', columns='Model', values='Value')\n",
        "\n",
        "    # Ensure consistent order of categories\n",
        "    pivot_df = pivot_df.reindex(categories)\n",
        "\n",
        "    pivot_df.plot(kind='bar', ax=ax, width=0.8)\n",
        "\n",
        "    # Customize plot\n",
        "    ax.set_title(f'Model Performance Comparison: {metric.replace(\"_\", \" \").title()}', fontsize=16, pad=20)\n",
        "    ax.set_ylabel('Score', fontsize=12)\n",
        "    ax.set_xlabel('Category', fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=0, labelsize=10)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "    ax.legend(title='Model', fontsize=10)\n",
        "    ax.set_ylim(0, 1.0)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', fontsize=8, padding=3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    file_name = f'{metric}_comparison.png'\n",
        "    plt.savefig(file_name)\n",
        "    print(f\"Chart saved as {file_name}\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bxJxeJwAmLW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}